---
title:  "[논문 리뷰] CoCa: Contrastive Captioners are Image-Text Foundation Models"
categories: [AI Tech, Computer Vision, Paper Review]
tags: [CoCa]
typora-root-url: ../
---

Contrastive learning와 ontrastive learning을 통합한 CoCa 논문 리뷰를 하였다.  
<br>  

# Abstract

위 논문에서는 이미지와 텍스트 관계를 학습한 Encoder 단위의 Contrastive learning과 이미지를 넣으면 캡션 생성하는 Encoder-Decoder 단위의 generative learning 구조를 통합한 Pretrained foundation model을 제안하였다.

이 모델은 CoCa(Contrastive Captioners are Image-Text Foundation Models)라고 불리며 Computer Vision 분야 다양한 Task에 대해서 SOTA를 달성하였다.  
<br>  


# Related Work

## (1) Vision Pretraining: Single encoder

Single Encoder란 하나의 Encoder를 통해 데이터를 하나의 고정된 벡터로 압축하는 방식이다. Single Encoder는 Cross entropy loss로 이미지를 효과적으로 분류할 수 있다. 주로 ImageNet과 같은 큰 규모의 annotation된 데이터셋으로 훈련한다.

Single Encoder의 장점으로는 일반적인 Feature를 잘 추출할 수 있으며, downstream task에 transfer되어 사용될 수 있다.

하지만 Output이 Discrete한 Class 벡터, 즉 시퀀스를 다룬 것이 아니기 때문에 자연어 형식으로 넘어갈 수 없는 단점이 있다.

Single Encoder 모델로는 ConvNets인 VGG, Resnet, Transformer 기반 모델 BEiT 등이 있다.

<br>  


## (2) Vision-Language Pretraining

주로 Fast(er) RCNN을 통해 Visual representation을 추출한 뒤 Language Model과 결합하는 방식을 사용한다. 

관련 모델로는 LXMERT, UNITER, ViLBERT 등이 있다.  
<br>  


## (3) Image-Text Foundation Models: Dual encoder

Dual encoder는 텍스트 그대로 Encoder를 2개 사용한 모델이다. 관련 모델로는 CLIP등이 있다. CLIP은 웹 상에 존재하는 이미지-텍스트 쌍을 통해 이미지와 텍스트의 관계를 학습한 모델이다. 모델 구조는 image encoder와 text encoder가 각각 unimodal로 학습되는 Dual encoder 구조가 되며 각 output을 embedding space로 매핑하여 contrastive loss를 적용하는 구조이다.

Dual encoder의 장점으로는 이미지 임베딩 뿐만 아니라 텍스트에 대한 임베딩이 동일한 latent space에 담길 수 있다는 것이다. 또한 zero-shot classification과 같은 understanding 관련 성능이 좋았다.

하지만VQA(visual question answering)과 같은 이미지 각 부분에 대한 captioning/reasoning 성능이 떨어지는 문제가 발생하였다.  
<br>  


## (4) Image-Text Foundation Models: Encoder-Decoder

generative learning관련 부분을 다루며 encoder-decoder 구조를 가진다. Encoder는 이미지 데이터를 숨겨진 표현으로 인코딩하고, Decoder는 이를 이용해 텍스트 데이터를 생성하며 Language Model Loss를 최적화하는 방식이다. 관련 모델로는 SimVLM 등이 있다. 

Encoder-Decoder의 단점으로는 text에 대한 unimodal encoder가 부재하기 때문에 입력 데이터가 이미지가 아닌 텍스트인 경우에는 image 없이 text-only representation을 생성할 수 없다. 

 또한 image encoder를 사용하여 이미지 데이터를 표현하는 벡터를 생성하지만 이미지의 시각적인 정보를 완전히 이해하지 못했기 때문에 제대로 된 이미지를 표현하는데 성능이 떨어지는 문제가 있다.  
<br>  


따라서 논문에서는 (3), (4)와 같은 문제를 해결하기 위해 single-encoder, dual-encoder, encoder-decoder 방식을 융합하고, 학습 과정도 단일화하여 image-text 관계성을 기반으로 end-to-end 최적화가 가능한 알고리즘을 만들고자 한다.

![image](https://user-images.githubusercontent.com/89712324/227085974-fb99b6e8-26a1-4433-81ee-90ac666f7cbf.png)  
<br>  


# Approach

## (1) Single Encoder Classification

Single Encoder Classification에서는 label된 데이터셋이 필요하다. 이 데이터셋은 discrete class vector로 되어있으며 cross entropy loss를 적용하여 최적화한다.

![image](https://user-images.githubusercontent.com/89712324/227086024-a92ce650-e6da-49bf-9770-7a4e7a60df1c.png){: width="300"}

단점으로는 class vector에 대해 학습하는 것은 label 정보를 natural language로 간주하지 않고 discretized label로 간주하기 때문에 language와 학습이 불가능하다. 이를 해결하기 위한 방법이 Contrastive Learning이다.  
<br>  


## (2) ****Dual-Encoder Contrastive Learning****

Dual-Encoder Contrastive Learning은 label된 데이터셋이 필요가 없다. 2개의 인코더를 한 embedding 공간 내에서 contrastive learning하기 때문이다. Contrastive learning은 contrastive loss를 적용하여 모델을 최적화한다. 

![image](https://user-images.githubusercontent.com/89712324/227086050-0fe4816f-8553-4b07-bead-0bea1ad48393.png)

image-to-text, text-to-image의 합으로 contrastive loss를 구한다.  
<br>  


## (3) ****Encoder-Decoder Captioning****

Encoder-Decoder Captioning은 이미지를 보고 어떤 이미지인지를 언어로 설명해주는 모델이다. 일반적으로 CNN과 RNNs이 결합된 형태를 사용한다.

Encoder는 이미지를 인코딩해서 이미지의 latent encoded feature를 출력하고 Decoder에서는 Encoder에서 추출한 특징을 기반으로 가능한 모든 문장 중에서 실제 입력 문장과 가장 유사한 후보를 선택하는 과정에서, 선택된 문장이 실제 입력 문장일 확률을 최대화 시킨다.

![image](https://user-images.githubusercontent.com/89712324/227086068-79a2acb7-3733-4148-8321-55b01361d443.png){: width="350"}

![image](https://user-images.githubusercontent.com/89712324/227086087-023fdc0a-15ef-4437-9a6b-521ef38050ce.png){: width="500"}

sequence to sequence 구조에서는 Decoder의 학습 효율성을 위해 Teacher forcing을 수행한다.

![image](https://user-images.githubusercontent.com/89712324/227086106-8fa81a2a-85c4-4501-aba5-8772b79de406.png)

Teacher forcing이란 target word(Ground Truth)를 디코더의 다음 입력으로 넣어주는 기법이다. teach forcing을 적용하지 않으면 이전 예측을 고려해주는 Decoder의 장점이 잘못된 예측이 입력되었을 때 엄청난 단점이 되어버린다.

장점으로는 학습이 빨라지게 되지만 단점으로는 Inference 단계에서는 Ground Truth를 제공할 수 없다는 것이다. 하지만 이러한 단점이 큰 영향을 미치지 않는다는 연구결과가 나와있다고 한다. (T. He, J. Zhang, Z. Zhou, and J. Glass. Quantifying Exposure Bias for Neural Language Generation (2019), arXiv.)  
<br>  

## (4) Contrastive Captioners Pretraining

Captioning은 conditional likelihood를 최적화한다. 즉, 이미지 또는 비디오와 같이 입력과 출력인 캡션 사이의 관계를 보고 이 관계를 사용하여 모델을 최적화한다.

반면 Contrastive learning은 unconditional likelihood를 최적화한다. 즉, 이미지, 비디오, 오디오 등 형식에 관계없이 모든 입력과 출력 간의 관계를 고려하여 모델을 최적화한다.

CoCa모델은 Dual-Encoder Contrastive Learning과 Encoder-Decoder Captioning의 방식을 합친 모델이다.

![image](https://user-images.githubusercontent.com/89712324/227086137-f2be571f-12d1-4f87-8526-f135748e5a52.png){: width="400"}

Contrastive Loss와 Captioning Loss를 합친 loss를 기반으로 최적화를 진행한다. 위 loss기반으로 학습하기 위해서는 captioning과 contrastive learning이 동시에 진행되어야한다.

단일 구조의 Decoder를 사용하게 되면 하나의 모델에서 두 가지의 방식을 학습할 수 없다. 따라서 Decoder layer를 Unimodal Text Decoder과 Multimodal Text Decoder 절반으로 나누어 학습을 진행한다.

![image](https://user-images.githubusercontent.com/89712324/227086190-58d5266f-c7f3-48bc-b844-d796bb65096a.png){: width="500"}  
<br>  


### **Unimodal Text Decoder**

Unimodal Text Decoder에서는 Text를 Input으로 받아 임베딩 벡터로 인코딩하고, text encoder에서 나온 Output CLS토큰을 기반으로 이미지 인코딩 결과와 cross-attention을 진행한다. CLS토큰과 Unimodal Text Decoder의 CLS토큰 간의 contrastive loss를 계산한다는 것이다. CLS토큰은 입력된 문장 전체에 대해 Text embedding 역할을 하는 토큰이다.

즉, Unimodal Text Decoder는 텍스트 자체를 이해할 수 있는 레이어이며 Text Representation을 학습한다.  
<br>  


### **Multimodal Text Decoder**

Multimodal Text Decoder 일반적인 생성 모델 방식이며 Image Encoder에서 나온 feature를 활용하여 가능한 text 중에 실제 text와 가까운 문장의 확률을 최대화한다.

즉, Multimodal Text Decoder는 Captioning 모델처럼 이미지와 텍스트를 이해할 수 있는 레이어이다.  
<br>  


### **Image Encoder**

입력 받은 이미지를 이미지를 인코딩해서 이미지의 latent encoded feature를 출력한다. Encoder의 Output에는 CLS 토큰과 같이 이미지 전체에 대한 정보를 인코딩하는 부분이 존재한다.  
<br>  


### Attention pooling

Contrastive loss와 같이 Understanding을 기반으로 하는 경우 단일 token에 각 modality 전체에 대해 요약하는 형태(single pooled image embedding)가 일반적이지만, generation task와 같은 downstream tasks의 경우 단순히 이미지 전체, 텍스트 전체를 요약하고자 하는 것이 아니기 때문에 많은 visual token이 사용되는 것이 좋은 성능을 보장할 수 있다.

따라서 Contrastive learning, mualtimodal text decoder에 사용되는 attention pooler를 정의하였다.   
<br>  


# CoCa for downstream tasks

Pretrained CoCa 모델은 image/text에 대해 다양한 zero-shot transfer가 가능하다. CLIP과 같은 Classification부터, image and text retrieval, video and text retreival 등이 있다.

또한 Pretrained CoCa Encoder을 통해 텍스트에서 feature를 추출할 수 있다. 각 task에 대해 frozen encoder를 사용하고, attention pooler를 학습하여 인코딩된 feature를 적절히 query할 수 있도록 하면 인코딩된 representation을 다양한 task에서 사용할 수 있다.   
<br>  


# Experiments

![image](https://user-images.githubusercontent.com/89712324/227086226-ca560b52-99af-4bfd-b4de-6f54b4fec4cd.png)

각 task의 기존 SOTA 성능을 노란색으로 표현했을 때, CoCa가 전반적으로 성능들을 뛰어넘는 것을 확인할 수 있다.  
<br>  


![image](https://user-images.githubusercontent.com/89712324/227086260-1ffd90e4-5d35-4a97-8dd4-a4b034621f1e.png)
CoCa를 fronzen했을 때와, finetuned했을 때 모두 기존 모델보다 높은 성능을 나타내고, SOTA 성능을 모두 뛰어넘었다는 것을 알 수 있다.

![image](https://user-images.githubusercontent.com/89712324/227086290-3bce65be-680a-4739-a493-0e95e58b617e.png)  
<br>  


![image](https://user-images.githubusercontent.com/89712324/227086311-61a4af5a-9964-4694-9810-49f9558d1342.png)

Flickr와 MSCOCO에 대한 image-text retrieval 부분에서도 CoCa 모델이  높은 성능을 냈다.  
<br>  


![image](https://user-images.githubusercontent.com/89712324/227086327-41a00cf3-4d4e-4045-afde-c833160d4eea.png)

또한 domain generalization에 관련된 dataset에 대해서도 zero-shot 성능이 매우 좋은 것을 확인할 수 있다.
