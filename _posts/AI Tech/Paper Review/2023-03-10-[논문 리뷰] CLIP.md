---
title:  "[논문 리뷰] CLIP: Learning Transferable Visual Models From Natural Language Supervision"
categories: [AI Tech, Computer Vision, Paper Review]
tags: [CLIP]
typora-root-url: ../
---

이미지와 텍스트의 관계를 학습한 CLIP에 대해 논문 리뷰를 해보았습니다.  

처음으로 직접 논문을 읽고 리뷰를 해보았기 때문에 서툰 점이 많을 것 같습니다. 잘못된 부분이 있으면 수정하도록 하겠습니다.  

그럼 리뷰 시작하겠습니다.  
<br>  

## Abstract

현대 컴퓨터 비전 학습 시스템은 레이블된 데이터 셋으로 훈련하기 때문에 모든 시각적인 개념을 명시하기 위한 라벨이 필요합니다. 그렇기에 일반성과 편리성에 한계가 있습니다.

이를 해결하기 위해 논문에서는 인터넷에 존재하는 4억 쌍의 이미지 텍스트 쌍을 이용해서 SOTA 이미지를 학습할 수 있는 효율적이고 확장 가능한 방법인 CLIP을 제시하였습니다.

이 작업은 어떤 캡션(텍스트)이 어떤 이미지와 관련있는지를 예측하며 자연어을 시각적 개념을 설명할 수 있어 zero-shot transfer을 가능하도록 합니다.  
<br>  


## Introduction and Motivating work

GPT-3와 같은 Text-to-Text 모델은 task-agnostic 구조로 zero-shot transfer가 가능하도록 하며, crowd-labeled dataset을 이용한 모델보다 좋은 성능을 보입니다.

하지만 Vision 분야에서는 여전히 crowd-labeled dataset을 이용한 task-specific 학습 방법이 주를 이룹니다.

Vision 분야의 선행 연구들이 있었지만 자연어 supervision을 적용한 방법은 드물고, zero-shot 부분에서 매우 낮은 성능을 보였습니다.

그렇기에 이 논문에서는 컴퓨터 비전분야에 이미지 표현을 위한 자연어 supervision을 적용하여 zero-shot transfer가 가능한 task-agnostic 방법을 제시하려고 합니다.  
<br>  


## Approach

### 1. Natural Language Supervision

CLIP의 idea로는 자연어 supervision을 통해 이미지 인지를 배우는 것입니다. 즉, 자연어를 training signal로 이해하는 것입니다.

자연어 supervision은 두가지 장점이 있습니다.

첫번째로는 인터넷에 존재하는 대량의 텍스트 데이터를 이용하기 때문에 각 데이터에 라벨링을 할 필요가 없습니다.

두번째로는 단지 표현을 학습하는 것이 아니라 표현과 언어를 연관지어 학습하므로 zero shot transfer가 가능합니다.  
<br>  


### 2. Creating a Sufficiently Large Dataset

**기존 데이터셋의 한계**

MS-COCO, Visual Genome 데이터셋은 품질이 좋지만 양이 매우 적습니다.

YFCC100M 데이터셋은 양이 많지만 메타데이터가 거의 없고, 품질이 일정하지 않습니다.

따라서 이 논문에서는 인터넷에서 4억 개의 이미지-텍스트 쌍을 수집하여 데이터셋을 구축하였고, 이 데이터셋을 WIT(WebImageText)라고 명명했습니다.  
<br>  


### 3. Selecting an Efficient Pre-Training Method

![image](https://user-images.githubusercontent.com/89712324/224334167-efc2d8a8-4df5-43b0-beb5-516ad37464ae.png){: width="500"}

한 배치에 N쌍의 텍스트-이미지 데이터가 있다고 해봅니다. CLIP은 NxN의 행렬 중 가능한 쌍을 예측하는 것입니다. NxN의 행렬을 만들기 위해서는 멀티모달 임베딩 공간을 학습해야합니다. 

이미지를 Image Encoder에 통과시켜 하나의 벡터를 만들고, 각 추출된 벡터에 가중치 행렬을 곱하고 L2-normalization을 수행하여 임베딩합니다.

각 이미지를 설명하는 텍스트는 Text Encoder에 통과시켜 하나의 벡터로 만듭니다. 텍스트 토큰 중 문장의 끝을 나타내는 [EOS] 토큰의 벡터값을 텍스트 전체의 벡터로 학습시킵니다. 그 다음 가중치 행렬을 곱하고 L2-normalization을 수행하여 임베딩합니다.

그 다음 두 백터를 내적한 NxN의 행렬에서 정답 쌍(대각원소)의 코사인 유사도를 최대로 만들고, 오답 쌍(대각원소를 제외한 원소들)의 코사인 유사도는 최소로 만드는 방향으로 symmetric cross entropy loss로 학습합니다. 코사인 유사도가 최대로 만들어지는 부분은 N개, 최소로 만들어지는 부분은 N^2 - N 개입니다. 

![image](https://user-images.githubusercontent.com/89712324/224334264-3c15deaf-8c62-404d-8394-4e10a8797051.png){: width="400"}
<br>  


### 4. Choosing and Scaling a Model

**Image encoder**

1. ResNet-50: ResNet에서 약간 수정된 버전인 ResNet-D 버전을 사용하였고 Global Average Pooling을 Attention Pooling으로 대체하였습니다.
2. ViT: Vision Transformer 구조를 거의 그대로 적용하며, layer normalization을 추가하였습니다.

**Text encoder**는 Transformer 구조를 사용하였습니다. 계산 효율성 측면 때문에 max sequence length는 76으로 제한했습니다. 

CLIP의 성능이 Text encoder의 capacity에 민감하지 않기 때문에 모델의 width만 스케일링하였고, depth는 스케일링하지 않았습니다. 

<br>  

### 5. Training

이 논문에서는 5개의 ResNet과 3개의 Vision Transformer를 학습시켰습니다. 

ResNet으로는 ResNet-50, ResNet-101 그리고 EfficientNet 스타일의 모델 3개를 더 학습하였습니다.

Vision Transformer에서는 ViT-B/32, ViT-B/16 그리고 ViT-L/14를 학습하였고 모두 32 에포크만큼 학습하였습니다.

Adam 최적화를 사용하였고 cosine schedule로 학습률을 점차 감소시켰습니다.  
<br>  


## Experiments

### 1. Zero-Shot Transfer

### Using CLIP for Zero-Shot Transfer

![image](https://user-images.githubusercontent.com/89712324/224334329-477e8905-36d8-4d0e-afe2-927ee101d53a.png){: width="500"}

앞서 CLIP에서 Contrastive pre-training 하였습니다. 이미지와 텍스트를 벡터로 변환한 후 이미지 벡터와 텍스트 벡터 간의 유사도를 계산하여 유사도가 커지는 방향으로 학습하였습니다.

Zero-Shot 이미지 예측을 할 때 예측하고자하는 이미지를 인코딩하여 벡터로 만들고, 학습에 사용되었던 분류하고자 하는 object들을 자연어 형식 “a photo of a {object}” 과 유사한 형식으로 텍스트를 만듭니다. 이 텍스트들을 각각 인코더하여 벡터로 만듭니다. 그 다음 텍스트 벡터와 이미지 벡터의 유사도를 구하여 가장 유사도가 큰 값을 갖는 텍스트를 찾을 수 있습니다.

자연어 형식으로 텍스트를 만드는 이유는 object를 설명하는 자연어 형식이 object만 있는 입력값보다 성능이 더 좋기 때문입니다.

![image](https://user-images.githubusercontent.com/89712324/224334373-4a45d284-025a-4600-9615-f4e9a8cb6e57.png){: width="300"}

<br>  

### Analysis of Zero-Shot CLIP Performance

![image](https://user-images.githubusercontent.com/89712324/224334425-1fcfaebc-db2a-4143-b44e-41743e143689.png){: width="300"}

CLIP을 다양한 데이터셋을 통해 fully supervised baseline과 비교하여 성능을 측정하였습니다. 

27개 중 16개의 데이터셋에서 baseline보다 더 좋은 성능을 보였습니다. 

Standford Cars, Food101과 같은 fine-grained task에서는 더 좋은 성능을 보였고, STL10에서는 99.3%의 정확도록 SOTA를 달성하였습니다.

하지만 EuroSAT, RESISC45와 같은 생소하거나 세분화된 데이터셋에서는 Zero-Shot 예측의 성능이 좋지 않다는 것을  알 수 있습니다.

![image](https://user-images.githubusercontent.com/89712324/224334488-395fdc70-0970-4887-9a1a-3b2c328b9fb5.png){: width="300"}

CLIP의 zero-shot과 다른 모델의 few-shot을 비교해봤을 때, CLIP의 zero-shot 성능이 더 좋았습니다.

<br>  

### 2. Representation Learning

![image](https://user-images.githubusercontent.com/89712324/224334544-f1cfbd54-c2cf-4dbf-afe4-ced0eb277625.png)

지금까지 CLIP의 Task-learning capabilities를 분석하였습니다. Task-learning capability는 이전에 학습한 경험을 활용하여 새로운 문제를 해결할 수 있는 능력입니다. 

Representation learning은 데이터로부터 특징(feature)을 추출하는 방법론 중 하나입니다. 

Representation learning의 성능을 측정하는 방법으로는 각 데이터셋에 대해 마지막 classification 레이어를 fine-tuning 하는 방식을 사용합니다. CLIP은 Representation learning에서도 좋은 성능을 냅니다. 

<br>  

### 3. Robustness to Distribution Shift

![image](https://user-images.githubusercontent.com/89712324/224334711-988e1833-7b8e-4486-8b1b-88f1716f4489.png)

Distribution Shift란, 모델이 훈련 데이터셋과 테스트 데이터셋 간의 분포 차이에 적응하지 못하고 성능이 저하되는 현상을 말합니다. CLIP은 이미지를 의미적으로 학습하기 때문에 Distribution Shift에도 강합니다.

![image](https://user-images.githubusercontent.com/89712324/224334771-61d98ed4-ddec-4423-bb26-f8b219fd40fe.png)

<br>  

## 4. Comparison to Human Performance

사람과 비교하여 CLIP의 성능을 측정하였습니다.

Oxford IIT Pets 데이터셋을 사용하여 5명의 사람에게 3669개의 이미지를 주고 37종류의 강아지, 고양이 분류를 하도록 하였습니다.

결과는 사람보다 CLIP의 성능이 훨씬 높았습니다.

![image](https://user-images.githubusercontent.com/89712324/224334833-4a002ca5-233f-47fa-b65c-9e80732e7ce7.png){: width="500"}

<br>  

## Limitations

Zero-Shot CLIP이 SOTA 성능에 달성하기 위해서는 1000배 이상의 계산량이 더 필요합니다. 이는 현재 하드웨어적으로 불가능합니다.

CLIP은 차종, 꽃의 종, 인공위성 등 세부적인 task에서는 성능이 떨어집니다. 또한 객체 개수 세기 등 추상적이거나 systematic한 task에서도 성능이 좋지 못합니다. 따라서 사전학습 단계에서 학습하지 못한 새로운 종류의 task에서는 좋은 성능을 내지 못합니다.

CLIP은 MNIST에서 88%의 정확도로 상대적으로 낮은 성능을 보입니다. 손글씨 이미지와 유사한 데이터가 거의 없기 때문입니다. 이는 딥러닝의 일반화라는 문제를 여전히 해결하지 못하였다는 것을 알 수 있습니다.

인터넷에서 데이터를 수집하였기 때문에 사회적 편견들도 함께 학습되어 윤리적 문제가 발생합니다.

<br>  

## Conclusion

CLIP은 자연어와 이미지쌍을 학습하여 자연어로 시각적 개념을 설명할 수 있도록 하여 zero-shot transfer가 가능합니다.

결과적으로 CLIP은 ResNet-50과 같은 일반적인 비전 학습 방법보다 더 좋은 성능을 보여주며, 적은 양의 학습 데이터로도 잘 작동한다는 것을 알 수 있었습니다.

<br><br>  

이상 CLIP 논문에 대한 리뷰를 마치겠습니다.  
읽어주셔서 감사합니다.  