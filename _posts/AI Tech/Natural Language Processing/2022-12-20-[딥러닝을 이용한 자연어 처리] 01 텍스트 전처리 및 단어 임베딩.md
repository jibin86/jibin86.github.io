---
title:  "[딥러닝을 이용한 자연어 처리] 01 텍스트 전처리 및 단어 임베딩"
# excerpt: "16~20년 코딩 테스트 기출문제 유형 분석"
categories: [AI Tech, Natural Language Processing]
tags: [전처리, 임베딩, stopword, stemming, word2vec, fastText]
---
# 01 텍스트 전처리 및 단어 임베딩

## 1. 자연어 처리

- 컴퓨터를 통해 인간의 언어를 분석 및 처리하는 인공지능의 한 분야

### 적용 사례

- 문서 분류 - 스스로 판단하여 문서 분류 ex) 뉴스 포털
- 키워드 추출
- 감정 분석

### 자연어 처리 + 머신러닝 적용 사례

- 문서 요약 (요약본, 제목 생성)
- 기계 번역
- 챗봇

## 2. 텍스트 전처리

- 데이터 탐색
    - 데이터 통계치, 변수별 특징 등
    - 단어의 개수, 단어별 빈도수 등
- 데이터 전처리
    - 이상치 제거, 정규화 등
    - 특수 기호 제거, 단어 정규화 등
- 토큰화: 주어진 텍스트를 각 단어 기준으로 분리
    - 기준
        - 공백
        - 소문자 처리; 웬만하면 소문자로
        - 특수기호 처리; 물음표, 느낌표 등

### 단어의 개수 및 빈도수 확인

![image](https://user-images.githubusercontent.com/89712324/215531633-b28fd3ec-8316-4f45-97e4-e0a1e5b4b5b0.png)

- 카운터는 딕셔너리
- line 각 한줄
- rstrip 맨 오른쪽 문자열 제거
- 대부분 단어 빈도수의 분포: 지프의 법칙
    - long tail

### **전처리1: 특수 기호 제거**

![image](https://user-images.githubusercontent.com/89712324/215531705-01f969ec-d612-4c43-a88f-8853fca08206.png)

- re: 정규표현식(정의하는 규칙을 가진 문자열 집합) 라이브러리
- 패턴을 담을 변수 만들기 regex =  re.compile(‘[^a-z A-Z]’)
    - a-z까지 문자, 띄어쓰기, A-Z까지 문자 형태만 잡아낼거야
- 패턴에 맞지 않는 문자 제거 regex.sub(’교체후‘, ‘원본문자열’)

### **전처리2: Stopword 제거**

문법적인 기능을 지닌 단어(he, him, this) 및 불필요하게 자주 발생하는 단어(어디서나 쓰이는 단어, 특징적이지 않는 단어) 제거한다.

- nltk: 자연어툴킷 라이브러리, 전처리 도와줌
    
    ![image](https://user-images.githubusercontent.com/89712324/215531773-6a4e2444-11e1-4e0a-8e3d-6341050b2ba9.png)
    
    - import stopwords에서 stopwords가 정의되어있음
    - stopwords.words(’english’): 토큰화된 문장을 stopwords를 통해서 stopwords 제거된 결과 얻을 수 있음, 리스트 형태 반환
    - 신규 stopword 추가
        
        ![image](https://user-images.githubusercontent.com/89712324/215531859-3f9cf25f-350a-40a0-8d8e-0502fd0b4517.png)
        

### **전처리3: Stemming:**

**Stemming:** 동일한 의미의 단어이지만, 문법적인 이유 등 표현 방식이 다양한 단어를 공통된 형태로 반환한다.

![image](https://user-images.githubusercontent.com/89712324/215531995-9f8374b8-a145-4882-9530-a0a42d92c976.png)

- PorterStemmer: 문법은 다르지만 동일한 의미로 분리하는 라이브러리
- stem(): 문자열을 넣으면 문자열의 원형을 반환

## 3. 단어 임베딩

1. 자연어의 기본 단위인 단어를 수치형 데이터로 표현
2. 단어 임베딩: 각 단어를 연속형 벡터로 표현
    
    ![image](https://user-images.githubusercontent.com/89712324/215532048-729e200c-56f6-4502-839a-aded2c360191.png)
    
3. 비슷한 문맥에서 발생하는 단어는 유사한 의미를 지님
    1. 유사한 단어의 임베딩 벡터는 인접한 공간에 위치
        
        ![image](https://user-images.githubusercontent.com/89712324/215532119-c46d1f39-ef22-4d98-9de5-1f6528d3cc9c.png)
        
    2. 임베딩 벡터 간 합과 차로 단어의 의미적 특징을 활용 가능
        
        ![image](https://user-images.githubusercontent.com/89712324/215532247-d7475873-705c-470b-bc87-3967c7800b75.png)
        

## 4. word2vec

- 주어진 문맥에서 발생하는 단어를 예측하며 단어 임베딩 벡터 학습한다
    
    ![image](https://user-images.githubusercontent.com/89712324/215532306-6e5863bf-4e5c-4a7e-815f-780bbd286655.png)
    
    - ‘엘리스는’을 주변 문맥을 통해 예측

![image](https://user-images.githubusercontent.com/89712324/215532361-74414679-5e95-4ed5-80ea-d02509360212.png)

- 학습데이터: 2차원 리스트이며 토큰화된 문장 사용
- Word2Vec 클래스
    1. min_count=1: 1보다 적게 발생하는 단어를 학습하지 않는다.
    2. window=2: 문맥의 범위; ‘엘리스는’을 앞 두 단어, 뒤 두 단어를 이용해서 ‘엘리스는’을 맞춘다.
    3. vector_size=300: 은닉층  노드의 개수
- build_vocab(doc): 각 단어에게 수치형 인덱스 할당
- train(학습할 데이터, total_examples=문서 개수, epoch=)

![image](https://user-images.githubusercontent.com/89712324/215532443-77aefd9a-49aa-417e-8aed-88e9813e2168.png)

- wv.most_similar(’엘리스는’): ‘엘리스는’과 가장 유사한 임베딩 벡터와 유사도 반환
- wv.similarity(’엘리스는’, ‘좋아한다’): 각각의 임베딩 벡터를 통해서 두 문자열의 유사도를 반환 (0~1)

## 5. fastText

- word2vec의 단점
    - word2vec: 학습 데이터에 존재한 단어의 벡터만 생성 가능
        
        ⇒미등록 단어(out-of-vocabulary, OOV) 문제: 학습 데이터 내 존재하지 않았던 단어 벡터는 생성할 수 없음<br>
        
        ![image](https://user-images.githubusercontent.com/89712324/215532498-828904fc-0b38-4241-afe3-41808926c1e5.png)

- 해결: 각 단어를 문자 단위로 나누어 학습한다.
    
    ![image](https://user-images.githubusercontent.com/89712324/215532564-8593ae7d-a54b-4c20-a9bc-85e81568c82b.png)
    
    - 학습 데이터에 존재하지 않았던 ‘좋아한다고’의 임베딩 벡터 생성 가능하다.

![image](https://user-images.githubusercontent.com/89712324/215532610-3ed55265-2d83-4438-aaea-1be7c174013f.png)

- 학습데이터: 2차원 리스트이며 토큰화된 문장 사용
- FastTest 클래스
    1. min_count=1: 1보다 적게 발생하는 단어를 학습하지 않는다.
    2. window=2: 문맥의 범위; ‘엘리스는’을 앞 두 단어, 뒤 두 단어를 이용해서 ‘엘리스는’을 맞춘다.
    3. vector_size=300: 은닉층  노드의 개수
- build_vocab(doc): 각 단어에게 수치형 인덱스 할당
- train(학습할 데이터, total_examples=문서 개수, epoch=)

![image](https://user-images.githubusercontent.com/89712324/215532666-bfbf7b38-3fa8-47b8-9636-191f87e85ff8.png)

- wv.most_similar(’엘리스는’): ‘엘리스는’과 가장 유사한 임베딩 벡터와 유사도 반환
- wv['좋아한다고']: 기존에 학습되지 않았던 임베딩 벡터를 학습된 단어들의 조합으로 임베딩 벡터를 반환해준다.
- wv.similarity(’엘리스는’, ‘좋아한다’): 각각의 임베딩 벡터를 통해서 두 문자열의 유사도를 반환 (0~1)