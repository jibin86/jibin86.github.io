---
title:  "[Computer Vision] Object Detection: YOLO v2 & YOLO9000 논문"
categories: [AI Tech, Computer Vision]
tags: [CS231n, CNN, YOLO]
---   
# Introduction

## 1. YOLOv1
- 1-stage object detector
- 각 grid cell 마다 지정된 개수의 bounding box를 예측한다. (x, y, w, h, pc)
- Class probabiliy: 동일한 grid cell에서 예측된 bbox일 경우 같은 class probability값을 가진다.
- 성능이 다른 object detector 보다 낮지만, 속도는 매우 빠르다.

<br>

## 2. Faster R- CNN
- 2-stage object detector
    1. region proposal network 사용하여 물체 유무, 위치 확인
    2. cnn을 통해 얻은 feature map에서 proposed region 기반한 ROI로 classification과 bbox 좌표를 구한다.
- 1-stage detector에 비해 성능은 높지만 속도가 매우 느리다.

<br>
<br>

# Main Contribution
- YOLOv2 제안: YOLOv1의 단점 개선; 연산은 빠르게, 정확도는 높게 개선하였다.

    ![image](https://user-images.githubusercontent.com/89712324/221517262-0b23b90d-56eb-4725-8492-7fdd66371d1a.png){: width="300"}

- YOLO9000 제안: 9000개의 객체를 구분할 수 있도록 하였다.  
    → 즉, 존재하지 않은 객체 클래스에 대한 예측도 가능해졌다.

  1. 세분화된 label 할당 가능
      
      ![image](https://user-images.githubusercontent.com/89712324/221517302-4afac7d1-8474-4695-8054-2e0927b5cfba.png){: width="300"}
      
  2. 새로운 분류모델인 Darknet-19을 통해 성능을 향상시켰다.

<br>
<br>

# YOLOv2 & YOLO9000
- YOLOv2
    1. Better: mAP 성능을 향상시키려고 노력하였다.
        
        ![image](https://user-images.githubusercontent.com/89712324/221517329-ebe31711-5440-45ab-a213-b6e90310b74b.png){: width="500"}
        
        - Better의 요소들을 하나씩 추가할 때마다 YOLOv2의 성능 증가
    2. Faster: 분류모델 Darknet-19 도입하여 성능을 향상시켰다.
- YOLO9000
    1. Stronger: Classification과 Detection data를 합쳐서 word tree를 생성하고 & Joint Training 진행하여 → 9000개 객체 분류 가능하도록 하였다.

<br>
<br>

## Model: YOLOv2

### 1. Better을 위해 사용한 방법
- batch normalization
  
  ![image](https://user-images.githubusercontent.com/89712324/221517376-900dfa97-306d-41ae-a433-cd81d12a16d6.png)
  
  - 배치 정규화를 통해 이전 레이어의 아웃풋인 다음 레이어의 인풋값의 분포를 고정시켰다.
  - 정규화로 overfitting 발생 확률을 줄였다.
  - dropout layer 대신 정규화를 사용하였더니 → mAP 2% 증가하였다.

<br>


- high resolution classifier
  1. 저해상도(224x224)로 cnn(darknet)을 pretrain하고, 마지막 10 epoch정도 고해상도(448*448)로 fine tuning을 하였다.
  2. object detection을 위해서 416x416 이미지로 fine tuning 하였다.

<br>

- convolutional with anchor boxes
  
  ![image](https://user-images.githubusercontent.com/89712324/221517416-53294fa9-10f6-4559-b500-eb6ab86654ac.png)
  
  1. 마지막 부분인 fc layer 2개를 모두 제거하고, cov network(1x1)을 넣어 prediction 하였다.
  2. grid cell마다 anchor box를 5개 사용하였다.              
      ⇒  mAP는 다소 감소했지만, Recall은 크게 증가함

<br>

- dimension clusters
  
  ![image](https://user-images.githubusercontent.com/89712324/221517550-f7120424-30df-4064-b769-4c5c22574730.png)
  
  - anchor box의 ratio, size를 k-means clustering을 통해 최적의 anchor box 탐색한다.
  - k-means clustering 기준: 1-IOU(box, centroid)
  - k=5일 때 가장 적합함
  - Anchor box 15개 사용한 것보다 Cluster IOU(k=5)를 한 것이 Avg IOU값이 높았음

<br>

- direct location prediction
  
  ![image](https://user-images.githubusercontent.com/89712324/221517595-1f731ccc-442a-48b0-99a7-8778c6ae3a53.png)
  
  - Anchor box의 각 좌표를 시그모이드 함수에 넣어 grid cell  내에 중심이 위치하도록 한다.

<br>

- fine-grained features
  1. Passthrough layer를 추가한다.
      - 고해상도 feature map(큰 객체에 용이)과 저해상도 feature map(작은 객체에 용이)을 합치는 레이어이다.
  2. 그 결과 mAP 1% 상승하였다.

<br>


- Multi-scale training
  1. FC 대신에 conv layer 사용하여 input size 변화가 가능하다
  2. 여러 사이즈의 이미지에도 적용가능한 detection 성능을 낼 수 있다
  3. 모든 batch 10개마다 random하게 input size 변경하여 학습한다.

<br>


- Experiment
  
  ![image](https://user-images.githubusercontent.com/89712324/221613456-55c82636-d3b4-43e3-b16c-4357efef7bbb.png)

<br>
<br>

  
### 2. Faster을 위해 사용한 방법
    
![image](https://user-images.githubusercontent.com/89712324/221517675-6efb2f4c-41f9-4025-8f7d-bb9916a3868b.png)

- CNN 모델인 DarkNet-19를 적용하였다.
    1. 19개의 conv layers 와 5개의 maxpooling layers로 구성
    2. 3x3 filter 사용 (더 깊은 네트워크 만들 수 있음)
    3. Network in Network 구조 이후 global average pooling 사용 → params 감소 효과
        
        ![image](https://user-images.githubusercontent.com/89712324/221517904-ddd2b5b9-a51d-4e6a-bafb-950af39a60a7.png)
        
    4. 1x1 filter로 feature representation을 더욱 압축
    - DarkNet-19는 VGG-16만큼 정확도가 높으면서, Google Net 처럼 빠른 모델이다.

<br>
<br>

  
## Model: YOLO9000

### 1. Stronger을 위해 사용한 방법
1. Hierarchical Classification 학습을 통해 Word Tree 생성한다.  
    => 공통 root를 갖는 label을 묶어 Word Tree 생성
2. Dataset Combination with WordTree
    - ImageNet + COCO 데이터셋 + ImageNet Detection 합쳐 9000개 class label 생성하였다.
3. Joint Classification and detection
    1. Detection dataset과 Classification dataset 을 oversampling으로 개수를 맞춘다.
    2. Detection dataset으로 classification & bbox regression의 loss propagate 진행한다.
    3. Classification dataset으로 classification의 loss만 propagate 진행한다.


<br>

- Experiment
    
    ![image](https://user-images.githubusercontent.com/89712324/221517942-8b7e9a98-5270-4d15-995f-d7d3798fc16f.png){: width="400"}
    
    - Word Tree를 Dartknet-19로 분류성능 검증하고, 
    - 같은 부모 노드로부터 나온 노드끼리 softmax 확률을 계산한다.  
        

    ![image](https://user-images.githubusercontent.com/89712324/221517990-3feac9ee-e8df-48de-a291-339fa99b7f48.png){: width="400"}
    
    특정 class만 예측 성능이 높았고, 일상용품 같은 경우는 성능이 매우 낮았다.

  
<br><br>

<br>

---

**참고 자료**

[고려대학교 산업경영공학부 DSBA 연구실 - [Paper Review] YOLO9000: Better, Faster, Stronger](https://www.youtube.com/watch?v=vLdrI8NCFMs)
