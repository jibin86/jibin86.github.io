---
title:  "[Computer Vision] 4. Introduction to Neural Networks"
categories: [AI Tech, Computer Vision]
tags: [CS231n, CNN]
---
스탠포드대학에서 발표한 CNN 강의 영상을 듣고 자료를 정리했습니다.
 
[Lecture 4 \| Introduction to Neural Networks](https://youtu.be/d14TUNcbn1k)<br>

# **Front propagation**

![image](https://user-images.githubusercontent.com/89712324/221592862-97d8f4e0-a402-4f5e-bc4a-b70d86cf3558.png){: width='200'}

- x, y, z값이 주어지고, 왼쪽에서 오른쪽 노드로 건너가며 연산이 진행

<br>


# **Back propagation**

- x, y, z가 f(x,y,z)에 어떠한 영향을 미치는지 알아보기 위해  **backpropagation** 연산을 활용
- 말 그대로 거꾸로 연산
- 뒤에서부터(마지막 연산부터) 편미분

    ![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbgwsqz%2FbtrOce3Kyu1%2FkKxjdmVLRWMkP12tSUnkcK%2Fimg.png)

<br>

- 행렬 **Back propagation연산**은 다음과 같다
    
    ![image](https://user-images.githubusercontent.com/89712324/221504532-6acf2fac-786e-4e68-9409-93ef161bc210.png)
    
<br>

- **Jacobian Matrix**
    
    ![image](https://user-images.githubusercontent.com/89712324/221593375-206562f2-4d57-43b8-8623-10ea496a5694.png){: width='600'}
    
    - 야코비 행렬은 해당 함수의 편도함수 행렬으로, local gradient * global gradient 연산을 할 때 필요한 매트릭스다.
    - Jacobian matrix크기는 4096 * 4096 으로 주어지면 minibatch 를 이용해 연산 시, 실용적이지 못하다. 따라서 연산하지 않고, 출력에 대한 x영향을 구할 때 max 연산을 통해 일부는 0으로 채운다.

<br>
<br>

# Neurual Network

![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F6c6Jx%2FbtrN9rWz60R%2FcjXexLAYN1sWag3wh1Ub21%2Fimg.png){: width='600'}

- 지난번에 다룬 Linear Classifier f = wx에 hidden layer 가 추가 되었을시 , 3072의 데이터가 w1와 곱해지고 h노드(hidden layer)에 들어가고 다시 w2 가중치 연산을 통해 10개의 출력값으로 나온다.
- hidden 노드에 따라 더 많은 Classifier를 생성할 수 있다.