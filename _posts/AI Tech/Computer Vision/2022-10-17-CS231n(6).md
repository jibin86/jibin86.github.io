---
title:  "[Computer Vision] 9. CNN Architectures"
categories: [AI Tech, Computer Vision]
tags: [CS231n, CNN]
---
스탠포드대학에서 발표한 CNN 강의 영상을 듣고 자료를 정리했습니다.

[Lecture 9 \| CNN Architectures](https://www.youtube.com/watch?v=DAOcjicFr1Y&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=9)<br>

# Lecture 9 - CNN Architectures

## 1. AlexNet (2012)

![image](https://user-images.githubusercontent.com/89712324/221584851-fd26e026-a71b-486a-8d1e-2121e94decb2.png)

- 최초의 Large Scale CNN 모델
- Convolution Layer와 Pooling Layer가 반복적으로 진행되는 패턴
- LeNet 보다 layer가 많다.
- 활성함수: ReLU
- 정규화: Dropout
- 최적화: SGD momentum
- if input 227 * 227 * 3,
  - output 55 * 55 * 96
  - param 개수 11*11*3*96 = 348348
    

<br>
<br>


## 2. VGGNet (2014)

![image](https://user-images.githubusercontent.com/89712324/221585013-c6533210-15a4-44e6-a5c0-aa0e8a486823.png)


- Conv Layer와 Pooling Layer가 반복적으로 진행되는 패턴
- AlexNet 보다 더 깊고(layer개수 많아짐) 3 * 3 size filter 사용
- GoogleNet 다음으로 성능이 우수하다.
- 다른 데이터에서도 특징 추출이 잘 되고, 일반화 능력이 뛰어나다.

<br>
<br>

## 3. GoogleNet (2014)

![image](https://user-images.githubusercontent.com/89712324/221585131-9b8618ac-2d98-4916-8d3e-4550047670f5.png)

![image](https://user-images.githubusercontent.com/89712324/221507994-b4260eee-6f37-40ae-b9a5-3420b3a3207e.png){: width='500'}

- 네트워크 더 깊어짐(22개의 layer)
- 연산 효율 증가
- Inception module 여러 개 쌓음
    - inception module 내부에 다양한 필터들이 병렬로 존재
- FC-Layer 없앰 → 파라미터 줄일 수 있음
- 문제: 레이어, pooling을 거칠 때마다 Depth가 늘어남 → 계산량이 커짐
- 방법: Bottleneck Block(1 * 1 conv) 사용 = 입력을 더 낮은 차원으로 보내 계산 복잡도를 줄일 수 있다.

![image](https://user-images.githubusercontent.com/89712324/221508047-ecaffbc4-0ad9-472a-ae47-235a45b0d06d.png)

- 보조 분류기(일반적으로 Average Pooling) → Gradient Vanishing 문제 해결

<br>
<br>

## 4. ResNet (2015)

![image](https://user-images.githubusercontent.com/89712324/221585409-3c37ba8d-f5a6-4ab6-9738-5c02e5fe8605.png)

- 깊은 네트워크 (152 layer)
- residual connection 사용
- 문제: 모델이 깊어질 수록 최적화 어려워 성능 저하되기 쉬움
- 해결: identity mapping = 얕은 네트워크를 가져와 input을 output으로 내보냄
    - residual block안에 identity mapping을 사용해 출력단으로 보내고, 실제 layer은 그 안에 추가된 F(x)변화량 만 학습
    - 최종 출력값: input X + 변화량(F(x))
- 마지막 FC layer 없앰 → 파라미터 수 줄임 → Global Average Pooling 사용하여 FC layer 하나만 갖게 함
- Conv layer 다음 Batch Normalization(BN)을 사용한다.
- 초기화는 Xavier를 사용하는데 2로 나눈 것을 쓴다. 이렇게 하면 SGD+Momemtum에서 좋은 성능을 가진다.
- learning rate 는 0.1로 설하고 loss가 줄어들지 않도록 조금씩 줄여준다.
- dropout은 사용하지 않는다.
- weight decay는 1e-5 이다

<br>
<br>

## 5. GoogleNet + ResNet (2017)

![image](https://user-images.githubusercontent.com/89712324/221508118-1d1e0a2f-491e-4c7a-9e45-97f23e42d99a.png)

- 2017년 기준으로 모델 성능을 비교했을 GoogleNet과 ResNet을 결합한 모델이 가장 좋은것을 확인할 수 있다.(원의크기 메모리 크기 , x축: 연산량, y축: 성능(정확도))
- VGG : 기장 많은 메모리를 요구하고 연산량도 많다.
- GoogleNet : VGG와 비교해서 성능은 비슷하지 메모리와 연산량을 줄였다.
- AlexNet : 연산량은 적고 성능도 낮다. 메모리또한 많이 요구된다.
- ResNet : 메모리나 연산이 중간이고 성능이 제일 좋다.