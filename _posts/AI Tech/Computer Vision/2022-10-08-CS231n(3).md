---
title:  "[Computer Vision] 3. Loss Functions and Optimization"
categories: [AI Tech, Computer Vision]
tags: [CS231n, CNN]
---
스탠포드대학에서 발표한 CNN 강의 영상을 듣고 자료를 정리했습니다.

[Lecture 3 \| Loss Functions and Optimization](https://youtu.be/h7iBpEHGVNc)<br><br>

# Lecture 3 | Loss Functions and Optimization

## loss function

- loss function
    - score에 대해 불만족하는 정도를 정량하기 위한 함수이다.
<br>

### SVM(Support Vetor Machine)의 loss function
  - **SVM**
      - 기계학습 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델이며, 분류, 회귀분석에 사용
      - 고차원에서 결정경계를 기준으로 데이터를 분류
  - **Support Vectors**
      - 결정 경계와 인접한 포인트들
  - **Margin**
      - 서포트 벡터와 Decision boundary 간의 거리
  - **Hinge loss function**
      - 허용가능한 오류 범위 내에서 가능한 최대 마진을 만들어야한다.

  - 고양이 클래스의 점수를 보면 cat보다 car의 점수가 높다 → 잘못 분류
    ![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJk1dO%2FbtrOgRs6s7u%2FW3IVvKN3KYjndzd3ctBZlk%2Fimg.png){: width='300'}
  
        
  
  - **SVM loss function 계산과정**
      
      ![image](https://user-images.githubusercontent.com/89712324/221503998-3eaab490-fd96-4263-b6ce-a334a9807df7.png)
      
      xi: 이미지, yi: 레이블, s: 레이블의 점수, 그 중 sj: 잘못된 레이블의 점수, syi: 제대로된 레이블의 점수
      
      잘못된 레이블 스코어에서 제대로 된 레이블간 스코어 차에 일을 더한값이 영보다 크면 그 값이 loss가 되고 0보다 작으면 0이 레이블이 된다
      
      ![image](https://user-images.githubusercontent.com/89712324/221596055-4c030b54-1b98-4c7c-aee4-18c43c41d943.png)
      
      세 개에 사진에 대해 Loss값(2.9, 0, 12.9)을 구하고 각 Li값을 합산하여 class의 개수(3)로 나눈다. → 5.27
      
  - **SVM의 loss function특징**
      1. 데이터에 둔감하고, score값보다 정답 클래스가 다른 클래스보다 큰지 작은지 중요하다
      2. loss는 0부터, 무한대까지 값을 가질 수 있다. → 특정 가중치가 너무 과도하게 커지지 않도록 정규화(Regularization)항 추가해 사용한다.
          
          ![image](https://user-images.githubusercontent.com/89712324/221596484-5a8bbec9-cc0f-47e8-8ed9-d684a9891f92.png){: width='500'}
          
      
      ![image](https://user-images.githubusercontent.com/89712324/221596669-9dee4dc0-318b-4179-aedf-a80fb680387d.png){: width='500'}
      
      - **L2 regularization**
          - 가중치가 전반적으로 작고 고르게 분산된 형태로 진행되어 과적합을 줄일 수 있다.
          - test set에 대한 일반적인 성능을 향상시킬 수 있게 된다.
            
Soft Max의 loss function
  - **soft max**
    - 신경망 출력층에서 사용하는 활성함수, 분류문제에 사용됨
    - 점수 벡터를 클래스 별 확률로 변환하기 위해 사용
  - **연산 과정**
    ![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbbv0Ji%2FbtrOfbFor0L%2FdIhnORRgUe6JGCyXQRKbfK%2Fimg.png)
        
    - 연산 과정은 주어진 스코어에 대해 exp 연산을 취해 값을 구하고, 그 값을 모두 더한 값을 해당 클래스 스코어에 나눠준다. 마지막으로 -log 연산을 취해 최종 확률을 계산한다.

## Optimization

- **optimization**
    - loss function을 최소화하는 Weight 찾기
    - w가 변할때 Loss가 얼마큼 변하는지 알아봐야한다.

방법 2가지

1. Random Search
    - w의 위치 값을 무작위로 포인트를 찾는 방법
    - 예측 정확도가 불안정 → 절대로 사용하지 않음
2. **Gradient Descent**
    - loss function의 경사를 따라 내려가는 방법
    - Numerical Gradient: 수치적으로 경사를 구하는 방법
        
        ![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbG719Q%2FbtrOgxO0ZFk%2FIr9S62rfjPszRrMXpWkZRk%2Fimg.png)
        
        - h를 0.0001으로 미세하게 지정한 후f(x+h)의 loss를 구하면 1.2532가 나오고 이둘의 차를 0.0001로 나눈 경사값은 -2.5를 얻었다.
        - 기울기가 음수라는건, 기울기가 w가 늘어나면 loss에 음의 영향을 준다는걸 의미한다.
        - 기울기가 늘어나면 loss에 양의 영향을 가지고 있다고 의미한다.
        - loss의 변화가 없는것은 기울기가 없다는 것이다.
        - 단점: 정확한 값이 아닌 근사값을 구한 것이라 값이 정확하지 않고, weight가 무수히 많을 경우 평가를 하기가 어렵고 값을 구하기에도 느리다.  
    - Analytic Gradient: 미분을 통해 경사를 구하는 법
        
        ![Untitled](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbGiF5z%2FbtrN67qJHSF%2FLWCb4ef5KH7MwIifdd9k80%2Fimg.png)
        
        - 미분식을 이용하면 다음과 같이 편미분값들을 구할 수 있다.
    - Gradient Check
        - 실제로는 Analytic Gradient을 쓰면 되고 계산을 정확하게 이뤄냈는지 검토과정에서 Numerical Gradient 방법을 활용한다.
    - 코드로 구현
    1. **Full Gradient Descent**
        
        ![image](https://user-images.githubusercontent.com/89712324/221596910-58e0427c-16f2-49a0-9a94-34e083ec9a57.png)
        
        loss function, data, weight 인자로 전달 → Gradient 구한다

        구한 gradient에 stepsize(learing rate)를 곱하고 기존 weight에 추가하여 Weight를 업데이트 한다.

        gradient 값만큼 weight를 감소시켜 주기 위해 앞에 마이너스 붙인다.  

    2. **Mini-Batch Gradient Descent**
        - 현실적으로 많이 사용되는 방법
        
        ![image](https://user-images.githubusercontent.com/89712324/221597011-98723083-894a-44ea-a97e-205967632fbe.png)
        
        training set의 일부만 사용해서 gradient을 계산하고, parameter을 업데이트해주는데, 계속 또 다른 training set의 일부를 이용해 parameter을 업데이트 하는 방법을 계속해서 반복하는 과정이다.

        Mini-Batch Gradient Descent 중 하나로 Stochastic Gradient Descent(SGD)가 있다.
