---
title:  "[Hands-On ML] 11장 심층 신경망 훈련하기(4) - 고속 옵티마이저"
categories: [AI Tech, Hands-On ML]
tags: [Hands-On ML, 옵티마이저]
---

지금까지 훈련 속도를 높이는 네 가지 방법을 보았다.  


1. 연결 가중치에 좋은 초기화 전략 적용하기

2. 좋은 활성화 함수 사용하기

3. 배치 정규화 사용하기

4. 사전훈련된 네트워크의 일부 재사용하기  


<br>


이번 포스트에서는 훈련 속도를 크게 높일 수 있는 또 다른 방법인 고속 옵티마이저에 대해 다뤄볼 것이다.

<br>


<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# 11.3 고속 옵티마이저

훈련 속도를 크게 높일 수 있는 또 다른 방법으로는 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용하는 것이다.

1. 모멘텀 최적화

2. 네스테로프 가속 경사

3. AdaGrad

4. RMSProp

5. Adam

6. Nadam


<br>
<br>

## 11.3.1 모멘텀 최적화

- 경사하강법  

  - **방식**:그레디언트가 속도로 사용되는 방법이다.

  - (갱신 가중치) <- (현재 가중치) - (학습률) * (가중치에 대한 비용함수의 그레디언트)  

    ![image](https://user-images.githubusercontent.com/89712324/220684195-93b0d1f5-0311-43df-88ec-53dc6d9dbeec.png){: width='100'}

  - 이전 그레디언트가 얼마였는지 고려하지 않는다.

  - 그레디언트가 아주 작으면 매우 느려지는 특징이 있다.

<br>

- 모멘텀 최적화

  - **방식**: 그레디언트가 속도가 아닌 가속도로 사용된 방법이다. 그렇기에 가속도 클 수록 더 빠르게 최적에 수렴할 수 있고 완만한 지역도 속도가 줄지 않고 빠르게 수렴할 수 있다.  

  - 모멘텀 벡터 m에 이전 그레디언트에 대한 정보가 포함되어있다.  

    ![image](https://user-images.githubusercontent.com/89712324/220685183-28aa2d4f-d155-4385-8c73-6286a966e749.png){: width='200'}

  - 모멘텀 m이 너무 커지는 것을 방지하기 위해서 모멘텀 β가 주어지며, β는 가속에 대한 공기 저항 부분에 해당된다.

  - 모멘텀이 0.9이면, 종단 속도는 η x 그레디언트 x {1-/(1-β)} 이므로 경사하강법보다 10배 빨라진다. 즉 β가 클 수록 공기 저항이 줄어든다.

  - 지역 최적점을 건너뛰도록 하는데 도움이 된다.

    - 모멘텀 때문에 옵티마이저가 최적값에 안정되기 전까지 건너뛰었다가 다시 돌아오고, 다시 또 건너 뛰는 방식으로 여러 번 왔다 갔다할 수 있다. 여기서 마찰 저항은 이런 진동을 없애주고 빠르게 수렴할 수 있도록 한다.  

  - 실제로 모멘텀 0.9에서 잘 작동하며 경사 하강법보다 거의 항상 더 빠르다.


<br>
<br>


### **케라스로 구현하기**

- learning_rate => η (학습률)

- momentum => β (모멘텀, 값이 클 수록 공기 저항이 줄어들며 모멘텀이 유지된다)



```python
import keras
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
```

<br>
<br>

## 11.3.2 네스테로프 가속 경사 (NAG)

- 모멘텀 최적화의 변종이고, 기본 모멘텀 최적화보다 거의 항상 더 빠르다.

- **방식**: 현재 위치가 θ가 아니라 모멘텀의 방향으로 조금 앞선 θ+βxm 에서 비용함수의 그레디언트를 계산한다.  

    ![image](https://user-images.githubusercontent.com/89712324/220838203-9ff62db8-7eee-4198-a498-2e3f9c4cd344.png){: width='300'}

- 일반적으로 모멘텀 벡터가 올바른 방향을 가리키기 때문에 원래 위치보다는 앞 단계의 그레디언트를 사용하는 것이 조금 더 정확하다.  

<br>
<br>


### **케라스로 구현하기**



```python
import keras
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)
```

<br>
<br>

## 11.3.3 AdaGrad

- 전역 최적점 방향에 따라 그레디언트 벡터의 스케일을 감소시킨다.   

  1. 각 차원<sub>i</sub>에 해당하는 그레디언트<sub>i</sub>의 제곱을 벡터s<sub>i</sub> 에 누적한다. 즉 가파를 수록 s<sub>i</sub>는 크게 커진다.

  2. 그레디언트 벡터를 √{s_i+ε}로 나누어 스케일 조정한다. (ε는 0으로 나누는 것을 막기 위한 값)  

      ![image](https://user-images.githubusercontent.com/89712324/220853928-08d20e32-ac5c-4db9-8b4d-204e13df34f8.png){: width='300'} 
<br>


- (**적응적 학습률**): 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소된다.

- 학습률 하이퍼파라미터η를 덜 튜닝해도 된다.  

  ![image](https://user-images.githubusercontent.com/89712324/220873247-dad73d65-ced1-43f8-ba47-fb78d30f11ae.png){: width='500'}
<br>


- **단점**: 학습률이 너무 감소되어 전역 최적점에 도달하기 전에 알고리즘이 완전히 멈추는 경우가 있다. 따라서 선형 회귀 같은 간단한 작업에는 효과적이지만, 심층 심경망에서는 사용하지 말아야한다!

<br>
<br>


## 11.3.4 RMSProp

- 모든 그레디언트가 아닌 가장 최근 반복에서 비롯된 그레디언트만 누적하는 방식으로 AdaGrad의 문제를 해결할 수 있다.  

    ![image](https://user-images.githubusercontent.com/89712324/220933051-29a1204a-58a5-472c-8931-d23045366326.png){: width='300'}

  - 보통 감쇠율β는 0.9로 설정하며, 기본값이 잘 작동하는 경우가 많아 튜닝할 필요는 없다.  

- 아주 간단한 문제를 제외하고는 RMSprop 옵티마이저가 AdaGrad보다 훨씬 성능이 좋다. Adam 최적화가 나오기 전까지 가장 선호하는 최적화 알고리즘이었다.


<br>
<br>


### **케라스로 구현하기**  



```python
import keras
optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

<br>
<br>


## 11.3.5 Adam과 Nadam 최적화
- Adam: 모멘텀 최적화와 RMSProp의 아이디어를 합친 것이다.

  - 모멘텀 최적화: 지난 그레디언트의 지수 감소 평균을 따르고 

  - RMSProp: 지난 그레디언트 제곱의 지수 감소된 평균을 따른다.



  ![image](https://user-images.githubusercontent.com/89712324/221097327-d103c072-9fbb-4d99-90bf-19683462f6f4.png){: width='400'}

  ![image](https://user-images.githubusercontent.com/89712324/221097454-f545654d-f447-4bf6-9c39-09c360c5b6d8.png){: width='400'}



- 1, 2, 5 단계 => 모멘텀 최적화와 RMSProp와 매우 비슷하다.

- 1단계 => 지수 감소 합 대신 지수 감소 평균을 계산한다.

- 3, 4 단계 => m과 s가 0으로 초기화 되므로 훈련 초기에 값을 m과 s값을 증폭시킨다.

- 모멘텀 감쇠 하이퍼파라미터β_1는 보통 0.9로 초기화하고, 스케일 감쇠 하이퍼파라미터β_2는 0.999로 초기화한다.  


<br>
<br>

### **케라스로 코드 구현**



```python
import keras
optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
```

<br>
<br>


**Adam의 변종 2가지 소개**

**AdaMax**  

- Adam의 L\_2 norm을 L_∞ norm으로 바꾼다.

- Adam보다 더 안정적이다.

**Nadam**

- Adam 옵티마이저에 네스테로프 기법을 더한 것이다. 그렇기에 Adam보다 조금 더 빠르게 수렴한다.

- Nadam이 Adam보다 성능이 좋지만 RMSProp가 나을 때도 있다.  

<br>
<br>


## 지금까지 소개한 옵티마이저 비교하기

![image](https://user-images.githubusercontent.com/89712324/221417828-e9e87a83-d656-428e-b64b-135621ef3da9.png)



## 11.3.6 학습률 스케줄링
- 학습률을 너무 크게 잡으면 학습이 빠르게 진행되지만 최적점 주변에서 발산하며, 너무 작게 잡으면 최적점에 수렴하는데 매우 오래 걸린다. 

  
  ![image](https://user-images.githubusercontent.com/89712324/222461792-ff90bde5-e4e8-4247-a4da-55b3cc893501.png)

- **학습 스케줄**: 훈련하는 동안 학습률을 감소시키는 전략
  - 큰 학습률로 시작하고 학습 속도가 느려질 때 학습을 낮추면 최적의 고정 학습률보다 좋은 솔루션을 빠르게 발견할 수 있다.

<br>

널리 사용하는 5가지 학습률 스케줄링에 대해 알아보자  

<br>

### 1. 거듭제곱 기반 스케줄링  

![image](https://user-images.githubusercontent.com/89712324/223139032-ff520f14-e3ac-42f6-a0ce-7be125f37b87.png){: width='100'}  
- t: 반복 횟수, η<sub>0</sub>: 초기 학습률, c: 거듭제곱수(일반적으로 1), s: 스텝 횟수 하이퍼파라미터
- 학습률은 각 스텝마다 감소된다.
- s번 스텝 후에 학습률은 η<sub>0</sub>/2로 줄어든다.
- s번 더 진행되면 학습률은 η<sub>0</sub>/3, η<sub>0</sub>/4, ...로 빠르게 감소하다가 느리게 감소된다.
- 튜닝해야할 하이퍼파라미터: η<sub>0</sub>, s, c

### 2. 지수 기반 스케줄링  
![image](https://user-images.githubusercontent.com/89712324/223140933-e1c55156-9119-4fe9-b98d-22dff6835fd0.png){: width='100'}  
- s번 스텝마다 학습률이 1/10배가 된다.  
- s번 스텝마다 일정하게 1/10배씩 감소한다.


### 3. 구간별 고정 스케줄링  
- 일정 구간의 에포크동안 일정한 학습률을 사용하고 다음 구간의 에포크 동안 작은 일정한 학습률을 사용한다.
- ex) (5에포크, η<sub>0</sub>=0.1), (50에포크, η<sub>1</sub>=0.001)
- 잘 동작할 수 있지만 적절한 학습률과 에포크 횟수의 조합을 직접 찾아봐야한다.

### 4. 성능 기반 스케줄링  
- 매 N 스텝마다 Validation loss를 측정하고 loss가 줄어들지 않으면 λ배 만큼 학습률을 감소시킨다.

### 5. 1 사이클 스케줄링
- 초기 학습률η<sub>0</sub>에서 η<sub>1</sub>까지 선형적으로 증가시키고, 나머지 절반 에포크 동안 선형적으로 학습률을 초기 학습률η<sub>0</sub>까지 줄인다. 마지막 몇 번의 에포크는 학습률을 선형적으로 소수점 몇 째자리까지 줄인다.
- 최대 학습률η<sub>1</sub>은 최적의 학습률을 찾을 때와 같은 방식으로 찾고, 10배 정도 낮은 값을 η<sub>0</sub>로 둔다.
- CIFAR10 이미지 데이터셋에서 이 방식을 사용하여 에포크 100번만에 91.9%의 정확도를 달성하였다.

### 스케줄링 정리  
저자는 성능 기반 스케줄링과 지수 기반 스케줄링이 둘 다 잘 작동했지만 튜닝이 쉽고, 최적점에 빨리 수렴하며 구현이 쉬운 지수 기반 스케줄링을 선호하였다. 그렇지만 1사이클 방식이 조금 더 좋은 성능을 내는 것 같다고 하였다.