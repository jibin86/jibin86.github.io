---
title:  "[Hands-On ML] 11장 심층 신경망 훈련하기(4) - 학습률 스케줄링"
categories: [AI Tech, Hands-On ML]
tags: [Hands-On ML, 학습률 스케줄링]
---

## 11.3.6 학습률 스케줄링
- 학습률을 너무 크게 잡으면 학습이 빠르게 진행되지만 최적점 주변에서 발산하며, 너무 작게 잡으면 최적점에 수렴하는데 매우 오래 걸린다. 

  
  ![image](https://user-images.githubusercontent.com/89712324/222461792-ff90bde5-e4e8-4247-a4da-55b3cc893501.png)

- **학습 스케줄**: 훈련하는 동안 학습률을 감소시키는 전략
  - 큰 학습률로 시작하고 학습 속도가 느려질 때 학습을 낮추면 최적의 고정 학습률보다 좋은 솔루션을 빠르게 발견할 수 있다.

<br>

널리 사용하는 5가지 학습률 스케줄링에 대해 알아보자  

<br>

### 1. 거듭제곱 기반 스케줄링  

![image](https://user-images.githubusercontent.com/89712324/223139032-ff520f14-e3ac-42f6-a0ce-7be125f37b87.png){: width='100'}  
- t: 반복 횟수, η<sub>0</sub>: 초기 학습률, c: 거듭제곱수(일반적으로 1), s: 스텝 횟수 하이퍼파라미터
- 학습률은 각 스텝마다 감소된다.
- s번 스텝 후에 학습률은 η<sub>0</sub>/2로 줄어든다.
- s번 더 진행되면 학습률은 η<sub>0</sub>/3, η<sub>0</sub>/4, ...로 빠르게 감소하다가 느리게 감소된다.
- 튜닝해야할 하이퍼파라미터: η<sub>0</sub>, s, c

### 2. 지수 기반 스케줄링  
![image](https://user-images.githubusercontent.com/89712324/223140933-e1c55156-9119-4fe9-b98d-22dff6835fd0.png){: width='100'}  
- s번 스텝마다 학습률이 1/10배가 된다.  
- s번 스텝마다 일정하게 1/10배씩 감소한다.


### 3. 구간별 고정 스케줄링  
- 일정 구간의 에포크동안 일정한 학습률을 사용하고 다음 구간의 에포크 동안 작은 일정한 학습률을 사용한다.
- ex) (5에포크, η<sub>0</sub>=0.1), (50에포크, η<sub>1</sub>=0.001)
- 잘 동작할 수 있지만 적절한 학습률과 에포크 횟수의 조합을 직접 찾아봐야한다.

### 4. 성능 기반 스케줄링  
- 매 N 스텝마다 Validation loss를 측정하고 loss가 줄어들지 않으면 λ배 만큼 학습률을 감소시킨다.

### 5. 1 사이클 스케줄링
- 초기 학습률η<sub>0</sub>에서 η<sub>1</sub>까지 선형적으로 증가시키고, 나머지 절반 에포크 동안 선형적으로 학습률을 초기 학습률η<sub>0</sub>까지 줄인다. 마지막 몇 번의 에포크는 학습률을 선형적으로 소수점 몇 째자리까지 줄인다.
- 최대 학습률η<sub>1</sub>은 최적의 학습률을 찾을 때와 같은 방식으로 찾고, 10배 정도 낮은 값을 η<sub>0</sub>로 둔다.
- CIFAR10 이미지 데이터셋에서 이 방식을 사용하여 에포크 100번만에 91.9%의 정확도를 달성하였다.

### 스케줄링 정리  
저자는 성능 기반 스케줄링과 지수 기반 스케줄링이 둘 다 잘 작동했지만 튜닝이 쉽고, 최적점에 빨리 수렴하며 구현이 쉬운 지수 기반 스케줄링을 선호하였다. 그렇지만 1사이클 방식이 조금 더 좋은 성능을 내는 것 같다고 하였다.