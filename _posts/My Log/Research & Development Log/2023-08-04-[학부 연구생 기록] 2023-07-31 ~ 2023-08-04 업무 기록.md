---
title:  "[학부 연구생 기록] 2023-07-31 ~ 2023-08-04 업무 기록"
categories: [My Log, Research & Development Log]
tags: [diffusion, log, 학부 연구생]
---   

학부 인턴을 하면서 배운 것과 실행한 내용들을 기록한 글이다.  
다음에 같은 일을 하게된다면, 이를 효과적으로 처리할 수 있도록 하고자 한다.

**한 일 목록**

1. Audio로 Image 편집하는 논문 있는지 조사하기
2. Dreambooth 논문 읽고 정리
3. Audio captioning dataset & Video captioning dataset 조사
4. Make A Stable Diffusion Video의 Inference 코드 실행 (보류)
5. text-to-audio 모델인 AudioLDM으로 실험하기

## 1. Audio로 Image 편집하는 논문 있는지 조사하기

- ECCV 2022, **Learning Visual Styles from Audio-Visual Associations**
    
    ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/0b0733d5-3524-48cb-b064-bce1c5e39bf6)

    [Paper: https://arxiv.org/pdf/2205.05072.pdf](https://arxiv.org/pdf/2205.05072.pdf)
    
    [Project: Learning Visual Styles from Audio-Visual Associations](https://tinglok.netlify.app/files/avstyle/)

    <br>
    
- CVPR 2023, **Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment**
    - **Learning Visual Styles from Audio-Visual Associations (ECCV)의 후속 연구**
    
    ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/46ed1707-945a-4997-bb9b-603478424b1e)
    
    ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/331e180f-6d59-4cbc-8522-e11fa15e722b)
    
    [Paper: https://arxiv.org/pdf/2303.17490.pdf](https://arxiv.org/pdf/2303.17490.pdf)

    [Project: Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment](https://sound2scene.github.io/)
    
    
<br>

## 2. Dreambooth 논문 읽고 정리

[DreamBooth project: https://dreambooth.github.io/](https://dreambooth.github.io/)

[DreamBooth paper: https://arxiv.org/pdf/2209.12330.pdf](https://arxiv.org/pdf/2209.12330.pdf)

[[논문 리뷰] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://jibin86.github.io/ai%20tech/computer%20vision/paper%20review/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-DreamBooth-Fine-Tuning-Text-to-Image-Diffusion-Models-for-Subject-Driven-Generation/)

<br>

## 3. Audio captioning dataset & Video captioning dataset 조사

[[데이터셋 조사] audio captioning dataset & video captioning dataset 조사](https://jibin86.github.io/my%20log/research%20&%20development%20log/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EC%A1%B0%EC%82%AC-audio-captioning-dataset-&-video-captioning-dataset-%EC%A1%B0%EC%82%AC/)

<br>

## 4. **Make A Stable Diffusion Video의** Inference 코드 실행 (보류)

[Github: https://github.com/lxj616/make-a-stable-diffusion-video](https://github.com/lxj616/make-a-stable-diffusion-video)

1. sjb 폴더 이동 후, git 클론
    - `git clone https://github.com/lxj616/make-a-stable-diffusion-video.git`

    <br>

2. conda 가상환경 생성 
    - `conda create -n make_a_stable_diffusion_video_sjb`
    - `conda activate make_a_stable_diffusion_video_sjb`

    <br>

3. inference 코드 실행
    - `python3 examples/research_projects/make_a_stable_diffusion_video/run_inference_video.py`
    - 에러 발생 ⇒ `ImportError: cannot import name 'StableDiffusionVideoInpaintPipeline' from 'diffusers'`
        - `diffusers` 의 conda 패키지와 저자의 `diffusers` 패키지와 이름이 같고, 구성이 매우 유사하여, conda에 설치된 `diffusers` 패키지를 지우고, 저자의 diffusers를 sys.path에 추가하였다.
            
            ```jsx
            import sys
            sys.path.append("/home/sjb/sjb/make-a-stable-diffusion-video/src")
            ```
            
        - `from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_video_inpaint import StableDiffusionVideoInpaintPipeline` : diffusers에서 StableDiffusionVideoInpaintPipeline를 가져오는데, 복잡하고, 오류가 많아 한 번에 경로를 지정해주었다.
            
            ```jsx
            ### 이전
            from diffusers import StableDiffusionVideoInpaintPipeline
            ```
            
            ```jsx
            ### 이후
            import sys
            sys.path.append("/home/sjb/sjb/make-a-stable-diffusion-video/src")
            from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_video_inpaint import StableDiffusionVideoInpaintPipeline
            ```
            
        - make_a_stable_diffusion_video 아래의 init.py를 추가했다.
            - 파이썬에서 디렉토리를 패키지로 인식하려면 그 디렉토리에 `__init__.py` 파일이 있어야 한다.
            `__init__.py` 파일은 해당 디렉토리를 패키지로서 인식시키고, 모듈을 포함한 패키지의 초기화를 담당합니다. 만약 해당 디렉토리가 패키지로서 인식되지 않으면, 상대 경로로 모듈을 찾는 데에 문제가 발생할 수 있다.
    - 라이브러리 설치        
        ```bash
        pip install torch
        pip install transformers
        pip install xformers
        pip install imageio
        ```
        
    - 모델 다운 받는 중
        
        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/759750cb-0b13-4b02-b429-25b78c1f068e)
        
        - 메모리 부족 오류 발생
            
            ```bash
            torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.81 GiB (GPU 0; 11.74 GiB total capacity; 7.54 GiB already allocated; 2.31 GiB free; 7.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
            ```
            
            - 총 11.74 GiB의 GPU 용량 중 이미 7.54 GiB가 할당되어 있거  2.31 GiB의 여유 메모리가 남아있지만, 현재 요청하려는 메모리는 추가 2.81 GiB이라 오류가 발생하였다.
                - ⇒ 실행 여부 확인이 목적이므로, 배치 크기를 줄여 GPU 메모리 사용량을 줄인다.
                    - 하지만 배치 크기는 1이다.
                        
                        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/db94565d-bfd3-4196-8782-f05d80fbc7fa)          
                        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/84494ab0-5544-4408-8db9-944c14fbb655){: width="200"}
                        
                    - Readme를 다시 읽어보기 적어도 24gb의 VRAM이 필요하다고 한다. 
                    ⇒ 용량 부족으로 코드 실행 보류하기로 한다.
                        
                        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/781ddcf3-430d-4943-be8f-162639c1af99)
                        
<br>

## 5. text-to-audio 모델인 AudioLDM으로 실험하기

- 내용: video dataset 조사한 것 중에 하나 선택해서 video caption을 input으로 했을 때 audio 결과가 어떤지 audioldm 모델로 결과 뽑고, video(caption에 해당하는)랑 audio(audioldm모델로 뽑은 결과) 를 합쳤을 때 두개가 sink가 얼마나 잘 안 맞는지 확인하기.
- 과정
    1. audioldm 환경 설정 & 실행
        
        ```python
        # Optional
        conda create -n audioldm python=3.8; conda activate audioldm
        # Install AudioLDM
        pip3 install audioldm
        
        ### Text-to-Audio Generation: generate an audio guided by a text
        # The default --mode is "generation"
        audioldm -t "A hammer is hitting a wooden surface" 
        # Result will be saved in "./output/generation"
        ```
        
        [GitHub - haoheliu/AudioLDM at dda0f54ab283ecdc1fe94ffc3182236cb8c343bf](https://github.com/haoheliu/AudioLDM/tree/dda0f54ab283ecdc1fe94ffc3182236cb8c343bf)
        
        - Input Text: `A hammer is hitting a wooden surface`
        - Output Audio (generated audio):
            
            <audio src="https://drive.google.com/uc?export=download&id=1As_mzk6gPKOH4Vml-vup0iTWvlrku77u" controls="true"></audio>
            
        <br>

    2. Video Dataset: Webvid에서 Video 4개 선정하고, Video의 caption을 넣고, Audio 생성
        - Input Text: `Travel blogger shoot a story on top of mountains. young man holds camera in forest.`
            - video

                <video height="180" width="288" controls>
                <source src="https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/21cab707-3f50-4b81-8759-b23ecaacc156"/>
                </video>

                
            - generated audio
                
                <audio src="https://drive.google.com/uc?export=download&id=1OSC22t1J4Ssn_B9bYqLH5fNO7MwljazB" controls="true"></audio>
                
        - Input Text: `Horse grazing - seperated on green screen`
            - video
                
                <video height="180" width="288" controls>
                <source src="https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/8c09dbe9-3fe7-4ddd-a153-a65d81bf7adf"/>
                </video>

                
            - generated audio
                
                <audio src="https://drive.google.com/uc?export=download&id=1E0T1rpJblpaU6bhurq6ue3vX8ksjVg1P" controls="true"></audio>
                
        - Input Text: `City traffic lights. blurred view`
            - video
                
                
                <video height="180" width="288" controls>
                <source src="https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/79f5ad73-7765-425c-bcd0-75f005ea80f2"/>
                </video>

                
            - generated audio
                
                <audio src="https://drive.google.com/uc?export=download&id=1Aqw6PbNNsIGyYU2CZ7zNWyv8UNFv6IyK" controls="true"></audio>
                
        - Input Text: `Young woman flexing muscles with barbell in gym.the coach helps her.`
            - video
                
                
                <video height="180" width="288" controls>
                <source src="https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/e2973aca-713c-4b1d-be5e-686f63f253b3"/>
                </video>

                
            - generated audio

                <audio src="https://drive.google.com/uc?export=download&id=194jb8OuKNHqFNeNOOdBFVBjhceXhCJTI" controls="true"></audio>

<br>