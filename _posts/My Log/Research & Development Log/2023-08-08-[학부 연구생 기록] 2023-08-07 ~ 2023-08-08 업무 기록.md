---
title:  "[학부 연구생 기록] 2023-08-07 ~ 2023-08-08 업무 기록"
categories: [My Log, Research & Development Log]
tags: [diffusion, log, 학부 연구생]
---   

학부 인턴을 하면서 배운 것과 실행한 내용들을 기록한 글이다.  
다음에 같은 일을 하게된다면, 이를 효과적으로 처리할 수 있도록 하고자 한다.

**한 일 목록**

1. Webvid 2.5M TRAIN/VAL 다운 받기
2. Multi-Concept Customization of Text-to-Image Diffusion 논문 리뷰
3. Multi-Concept Customization of Text-to-Image Diffusion 코드 실행 및 결과 확인
4. Dreambooth와 같이 Subject를 유지하면서 Video 생성하는 논문 있는지 조사

<br>

## 1. Webvid 2.5M TRAIN/VAL 다운로드

1. 가상환경 생성 & 실행
    - `conda create -n webvid_sjb
    conda activate webvid_sjb`
2. 다운로드에 필요한 패키지 설치
    - `conda install pandas numpy requests mpi4py`
3. Webvid 2.5M TRAIN 비디오 데이터 다운
    - `python download.py --csv_path results_2M_train.csv --partitions 1 --part 0 --data_dir ./data --processes 8`
        
        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/42779eef-46bc-4256-b83a-c3141d6d4bf4){: width="400"}

        
        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/9714e876-568d-4003-a11b-156180c615b8)

<br>       

## 2. Audioldm으로 오디오를 생성할 비디오 & 캡션 선정

![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/10e5564f-daf3-4c03-8fb4-f0e33df468ae)

<br>

## 3. Multi-Concept Customization of Text-to-Image Diffusion 논문 읽고 정리

[[논문 정리] Multi-Concept Customization of Text-to-Image Diffusion (Custom Diffusion)](https://jibin86.github.io/ai%20tech/computer%20vision/paper%20review/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Multi-Concept-Customization-of-Text-to-Image-Diffusion-(Custom-Diffusion)/)

<br>

## 4. **Multi-Concept Customization of Text-to-Image Diffusion** 코드 실행 및 결과 확인

[https://github.com/adobe-research/custom-diffusion](https://github.com/adobe-research/custom-diffusion)

### 4.1. 코드 실행에 필요한 파일 다운로드 & 가상환경 생성 & 실행

```bash
### custom-diffusion 깃허브 클론
git clone https://github.com/adobe-research/custom-diffusion.git
cd custom-diffusion

### stable-diffusion 깃허브 클론
git clone https://github.com/CompVis/stable-diffusion.git
cd stable-diffusion

### stable-diffusion의 사전에 정의한 environment.yaml 파일에 따라 가상환경 생성
conda env create -f environment.yaml
conda activate ldm
pip install clip-retrieval tqdm
```

- `conda env create -f`: 기존의 환경 설정 파일을 사용하여 새로운 가상 환경을 생성한다.
    - `-f` 옵션은 환경 설정 파일을 나타내며, 일반적으로 YAML 형식으로 작성된 파일이다.
- `conda create -n`: 새로운 빈 가상 환경을 생성한다.
    - `-n` 옵션 다음에 새로운 가상 환경의 이름을 지정한다. 이 방법은 환경 설정 파일을 사용하지 않고 직접 가상 환경을 구성하는 경우에 사용된다.

```bash
### 모델을 저장한 디렉토리 생성
mkdir -p models/ldm/stable-diffusion-v1/
cd models/ldm/stable-diffusion-v1/

### Download the stable-diffusion model checkpoint 
wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt
```

<br>

### 4.2. **Single-Concept Fine-tuning**

1. **Real images as regularization: regularization을 위한 이미지 생성**
    
    ```bash
    ## download dataset
    wget https://www.cs.cmu.edu/~custom-diffusion/assets/data.zip
    unzip data.zip
    ```
    
    ```bash
    ## run training (30 GB on 2 GPUs)
    bash scripts/finetune_real.sh "cat" data/cat real_reg/samples_cat  cat finetune_addtoken.yaml <pretrained-model-path>
    ```
    
    - fine-tuning **실행 시에 발생한 오류 사항들**
        1. `MisconfigurationException: You requested GPUs: [0, 1]
        But your machine only has: [0]`
            - `finetune_real.sh` 파일의 gpu 개수 수정
                
                ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/5bdbdee8-8209-42f0-bfbe-c6f3eeba641b)
                
                ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/8124738f-39c7-4f9a-88e8-0144cf103ac7)
                
                - ⇒ `-t --gpus 0, \`
                    - 이때, 쉼표를 지우면, 아래 코드에서 int를 split하는 오류가 발생한다. 
                    ⇒ 쉼표를 유지한다.
                        
                        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/475f673b-35cf-4462-a1ba-4d5b77a3af7d)
                        
                        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/d116a9b0-8069-49e4-8a6b-9c101e8e386c)
                        
        2. GPU 메모리를 모두 다 써 오류가 발생했다.
            
            ```bash
            RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.74 GiB total capacity; 7.01 GiB already allocated; 213.69 
            MiB free; 8.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid 
            fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
            ```
            
            ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/88e3aa97-1752-4f48-8069-cbc7bafcccb3)
            
<br>

## 4. Dreambooth와 같이 Subject를 유지하면서 Video 생성하는 논문 있는지 조사

- Dreambooth 구조 ⇒ for customizing
    
    [[논문 리뷰] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://jibin86.github.io/ai%20tech/computer%20vision/paper%20review/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-DreamBooth-Fine-Tuning-Text-to-Image-Diffusion-Models-for-Subject-Driven-Generation/) 
    
    - **Fine-Tuning** Text-to-Image diffusion
        
        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/51bb346a-eebb-41f7-8368-1e9cc9e4d2ab)
        
        1. low-resolution **text-to-image model**을 **fine-tuning**한다.
            1. **input image**와 **text prompt("A photo of a [T] dog”)** 쌍으로 diffusion model을 fine-tuning한다.
                - **text prompt**는 **unique identifier([T])**와 **class name(dog)**으로 구성된다.
            2. **class-specific prior preservation loss 적용**
                
                ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/bef3c5f9-3fe9-4fc3-aecf-870e89c91389)
                
                - 클래스 이름을 text prompt에 넣어도 (e.g., "A photo of a dog”) dog라는 클래스 prior가 유지될 수 있도록 위의 fine-tuning과 함께 학습한다.

            <br>

        2. super resolution components를 fine-tuning하여 높은 해상도 이미지를 얻는다.
            1. input images set의 **low-resolution과 high-resolution image쌍**으로 fine-tuning 
                
                ⇒ 작은 디테일에 대한 높은 정확도 유지 가능

        <br>
                
    - Inference
        
        ![image](https://github.com/jibin86/RealTimeFaceRecognition/assets/89712324/4d475fda-e380-47f9-be0a-feb8c7ebf599)
        
        - **unique identifier**를 다른 문장들에 넣고, **personalized text-to-image** 모델을 통해 이미지를 생성한다.

<br>

- 논문 조사:

    - Dreamix: Video Diffusion Models are General Video Editors
        
        [https://arxiv.org/pdf/2302.01329.pdf](https://arxiv.org/pdf/2302.01329.pdf)
        
    - Animate-A-Story: Storytelling with Retrieval-Augmented Video Generationd
        
        [https://arxiv.org/pdf/2307.06940.pdf](https://arxiv.org/pdf/2307.06940.pdf)
        
    - DreamBooth-V
    - Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts
        
        [https://arxiv.org/pdf/2305.08850.pdf](https://arxiv.org/pdf/2305.08850.pdf)
        
    - Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models
        
        [https://arxiv.org/pdf/2307.06940.pdf](https://arxiv.org/pdf/2307.06940.pdf)
        
    - Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators
        
        [https://arxiv.org/pdf/2303.13439.pdf](https://arxiv.org/pdf/2303.13439.pdf)
        
    - Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation

        [https://arxiv.org/pdf/2306.07954.pdf](https://arxiv.org/pdf/2306.07954.pdf)