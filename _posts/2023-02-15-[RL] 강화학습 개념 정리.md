---
title:  "[RL] 강화학습"
categories: [ML (Others)]
tags: [ML]
---
강화학습의 개념에 대해 간단히 정리해보았다. 강화학습이 무엇인지 살펴보자.
<br>

# 강화학습

## 구성 요소

![image](https://user-images.githubusercontent.com/89712324/219000547-e6b80d8f-9b0f-4fc3-9502-a2f6b033b78a.png)

- Agent(에이전트) = 차량
- Environment(환경) = 트랙
- Action(행동) = 조향, 속도
- Reward(보상) = 점수
- State(상태) = 카메라, 라이다

## 이익(Return)과 할인계수(Discount Fator)

![image](https://user-images.githubusercontent.com/89712324/219000592-d1edb55d-64d5-4a2e-a0fe-00b92da383a1.png)

- G(return, 이익)은 미래에 받게 될 보상의 총합이다.
- 강화학습의 궁극적인 목표는 **G(이익)이 최대가 되는 A(행동)을 찾는 것**이다.
    
    ![image](https://user-images.githubusercontent.com/89712324/219000663-c8f79e15-6f6e-4a30-bed0-ae2629abafad.png)
    
- **문제**
    - 가까운 미래와 먼 미래에 받게 되는 보상의 가치가 동등하게 계산된다.
    - 무한히 더하다보면 무한대가 될 수 있다.
- **해결 - Discount Factor**
    - Discount Factor를 보상에 Step(시간의 경과)의 개수만큼 곱하여 더하는 방식으로 Return을 정의한다.
    - Discount Factor의 범위는 0과 1사이이며 일반적으로 0.9에서 0.99사이 값을 활용한다.
    - 시간 간격이 매우 촘촘한 경우 0.99보다 큰 값을 활용하기도 한다.
        - ex) 딥레이서에서 감마를 높게 잡는다.
            - 1초에 15번 행동하니까 r10승 1초, 0.9^10 = 작아진다. 낮게 측정되는 경향이 있다.
            - 1초라는 가까운 미래가 작게 측정되지 않도록 높게 잡는다.

## MDP (Markov Decision Process)

강화학습 문제를 풀기 위한 환경의 기본 요소를 정의하는 공간이다.  

![image](https://user-images.githubusercontent.com/89712324/219000714-331c61cf-9d07-432b-82ac-246d6d22d1a1.png)

- <**S**(state), **A**(action), **P**(state transition), **R**(reward), **γ**(discount factor)> 와 같이 표현한다.
- 검은 화살표 = state transition Probability
분홍 화살표 = Reward
하늘색 화살표 = Policy
⇒ 하늘색 화살표(Policy)를 학습시켜야 한다.

## 강화학습 알고리즘 분류 - Agent의 구성 요소

![image](https://user-images.githubusercontent.com/89712324/219000752-3dd52eed-06d8-4d78-8e96-9dd871449d6f.png)

### 1. 정책함수 (Policy, π)

- Policy는 현재 상태 S에서 행동 확률 분포를 출력하는 모델이다.
- 수식으로 π(a|S) ≔ P(a|S) 와 같이 정의한다. (S상태에서 a행동을 취할 확률)
    
    ![image](https://user-images.githubusercontent.com/89712324/219000791-c1f4ca79-4635-47f9-98c8-8d21e4e4f1e4.png)
    

### 2. 가치함수 (Value)

- 미래 보상의 합이 어떻게 될지 평균치를 예상하는 모델이다.
1. **상태 가치함수 V(S) (State Value)**
    - 화살표를 따라갔을 때(action 했을 때) **현재 상태(S)에서의 가치**가 얼마나 되는지
        
        ![image](https://user-images.githubusercontent.com/89712324/219000845-cdba3d32-fe30-4eb2-848d-ede83398888c.png)
        
        ![image](https://user-images.githubusercontent.com/89712324/219000888-3d923402-45c2-4703-89b8-27d2bf62f84c.png)
        
2. **행동 가치함수 Q(S, A) (Q-Value)**
    - S상태에서 **A의 행동을 했을 때** 가치
        
        ![image](https://user-images.githubusercontent.com/89712324/219000915-97f6fb0f-29d8-4417-b4e2-24f69147b97d.png)
        

### 3. Model (환경을 예측하는 모델)

- 환경의 다음 State와 Reward가 어떨지에 대한 agent의 예상이다.
- State model - 환경의 **다음 State가 무엇일까**에 대한 확률
    
    ![image](https://user-images.githubusercontent.com/89712324/219000956-abc11617-6c85-4d65-b8a8-091459f4f50a.png)
    
- Reward model - 환경의 **다음 Reward가 무엇일까**에 대한 확률
    
    ![image](https://user-images.githubusercontent.com/89712324/219001003-3ef14659-9b14-4150-8ad9-600dd7b6dbea.png)
    

### 정책 반복 (Policy Iteration)

- 정책과 가치를 반복적으로 업데이트 하는 과정
- 두 녀석이 같이 학습을 하면서 반복하다보면 최상의 값으로 수렴할 것이다.
- 업데이트 과정
    
    ![image](https://user-images.githubusercontent.com/89712324/219001043-9ecda23c-1589-400a-9d2c-c61d8b867268.png)
    
    - Q1 값을 토대로 확률분포 π1를 뽑는다.
    - 행동을 하면 Q가 변하고, Q가 변하면 π를 뽑는다.
        - 현재상태에서 위로가면 10점을 받게될 것이다.
        - Q를 바탕으로 Policy는 위로 0.02, 아래로 0.08의 확률로 움직이게 될 것이다라는 확률분포를 출력한다.

## 강화학습 알고리즘 분류 - **Model-Free vs Model-Based**

![image](https://user-images.githubusercontent.com/89712324/219001091-4ec736a3-1cfd-4eec-a41e-6f546edb685f.png)

- 기준: Environment에 대한 model의 존재 여부
- Model의 장점
    - 자신의 acition에 따라서 Environment가 어떻게 바뀔 지 알기 때문에 실제로 행동하기 전에 미리 변화를 예상해보고 최적의 행동을 계획하여 실행할 수 있다.
- Model의 단점
    - Environment의 정확한 model은 보통 알아내기 어렵거나 불가능하다.
    - Model이 환경을 제대로 반영하지 않는다면 이 오류는 agent의 오류로 이어지게 된다.

## 강화학습 알고리즘 분류 - **Value-Based vs. Policy-Based**

- 기준: Value function과 Policy의 사용 여부
- **Value-based agent** : Value function만 학습하고 Policy는 암묵적으로 갖고 있는 agent
    - 장점: 데이터를 더 효율적으로 사용할 수 있다.
    - Value function이 완벽하다면 최적의 Policy는 자연스럽게 얻을 수 있다. (각 state에서 가장 높은 value를 주는 action을 선택하면 되기 때문)
    - ex) DQN
- **Policy-based agent** : Value function이 없이 Policy만을 학습하는 agent
    - 장점: 원하는 것에 직접적으로 최적화를 하기 때문에 더욱 안정적으로 학습된다.
    - Policy가 완벽하다면 value function은 굳이 필요하지 않다. (value function은 policy를 만들기 위한 중간 계산이기 때문)
    - ex) Policy Gradient
- **Actor-Critic agent** : Value function과 Policy를 모두 갖고 있는 agent

<br><br>
지금까지 강화학습 개념에 대해 알아봤다. 인간보다 똑똑해질 수 있는 최적의 결과를 낼 수 있다는 사실이 정말 흥미로웠다. 강화학습, 공부할 수록 더욱 궁금해지는 분야이다.