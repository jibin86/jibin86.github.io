---
title:  "[데이터 전처리] 데이터 전처리 방법 정리하기"
categories: [Data handling]
tags: [Normalization,Standardization, encoding, PCA, LDA]
---
데이터 전처리 복습을 하며 정리하였다.
<br>

1. Cleaning input data
2. Working with missing values
3. Normalization and Standardization
4. Handling Categorical data
5. High-dimensional data and the curse of dimensionality
6. Feature selection with filter and wrapper methods
7. Feature reduction with transformation

# Data Cleaning

## Handling Missing Values

### Finding and removing missing values

- .isnull().values.any() : True면 결측치 존재한다.
- .isnull().values.sum() : 6이면 6개의 결측치가 존재한다.

![image](https://user-images.githubusercontent.com/89712324/216985345-45957d65-0099-4947-a13c-95dbf4111382.png)

- .fillna(’example’) : 결측치를 example로 채우기

![image](https://user-images.githubusercontent.com/89712324/216985373-983a0568-179b-45d5-96b9-f31782bde4b6.png)

- .dropna() : 결측치를 포함하는 열 또는 행 제거하기
    - option
        - axis=0 or ‘index’: drop rows
        - axis=1 or ‘columns’: drop columns
        
        ![image](https://user-images.githubusercontent.com/89712324/216985448-8b80a767-e439-475e-82b7-d9bd028341b3.png)
        
- .mean() :  NaN을 mean으로 대체
    
    ![image](https://user-images.githubusercontent.com/89712324/216985496-aa74708a-def0-4097-8016-f8cddbdb8c3a.png)
    
- Imputor: 대체할 값을 예측하기 위해 사용한다.
    - scikit-learn에서는 SimpleImputor()을 호출하여 사용할 수 있다.
    - option
        - missing_values : the form of the missing values (ex. nan, 0, or n/a)
        - strategy: the impute method (the choices are mean, median, most frequent, and constant)
        - fill_value : optional argument to pass the constant when choosing “strategy=constant”
        
        ![image](https://user-images.githubusercontent.com/89712324/216985540-cae33f1b-d18d-4956-b92b-76a502ba6678.png)
        
        ![image](https://user-images.githubusercontent.com/89712324/216985577-878559b6-c55f-4b5b-9342-024fa67658ce.png)
        

## Binning

- .cut(): 수치형을 범주형으로 묶을 수 있다.
    - retbins=True: return the bins (mybin), default False

![image](https://user-images.githubusercontent.com/89712324/216985612-66b87f32-4f66-4499-915b-dbd36629c144.png)

## Feature scaling

- Normalization: 모든 값들이 0과 1사이의 값을 갖도록 스케일한다.
    
    ![image](https://user-images.githubusercontent.com/89712324/216985682-f3145dca-aed0-4466-a18d-4c1c59166043.png)
    
- Standardization: 모든 값이 평균이 0이고 분산이 1이 되도록 스케일한다.
    
    ![image](https://user-images.githubusercontent.com/89712324/216985745-e086feac-43f3-401d-8e7b-983f337486bb.png)
    

## Handling categorical data

- 범주형 데이터를 수치형 데이터로 바꿔준다.

### Ordinal encoding

- 순서가 있는 범주형 데이터를 순서가 있는 수치형 데이터로 변환해준다.

![image](https://user-images.githubusercontent.com/89712324/216985813-b7163194-42fc-4737-afb4-6a3bad575703.png)

![image](https://user-images.githubusercontent.com/89712324/216986237-ed74eef9-8775-4bec-be16-dba161ec1258.png)

### One-hot encoding

범주형 데이터를 0, 1 이진 데이터로 변환한다.

![image](https://user-images.githubusercontent.com/89712324/216985896-fc4c56a6-d414-408a-9a39-3d17b6b01fcf.png)

- 단점: 데이터 안에 존재하지 않는 범주는 추정할 수 없다. ex) Shoe Size 11

![image](https://user-images.githubusercontent.com/89712324/216985963-c84416e7-0ce9-422e-936f-be1bd2a0543a.png)

### Label encoding

- 범주형 데이터를 수치형 라벨로 변환한다.

![image](https://user-images.githubusercontent.com/89712324/216986485-1cbc2a9a-d222-491f-8118-82d69fb515be.png)

# Dimension reduction

차원 축소해야하는 이유

- 데이터의 차원이 높으면 차원의 저주가 발생
- 4차원 이상의 데이터는 시각화하기 어렵다.

차원 축소 방법

1. Selection: 가장 좋은 특성을 선택하고 나머지를 제거한다.
2. Transformation: 기존의 특성을 요약하여 새로운 특성을 만든다.

특성 선택 방법

1. Filter: 중요한 평가기법을 정하고 중요한 것들만 필터링한다.
2. Wrapper: 최적의 결과를 선택하기 전에 다양한 특성 조합들을 샘플링한다.

### Feature Filtering

중요한 평가기법을 정하고 중요한 것들만 필터링한다.

- 일반적인 방법
    - variance threshold
        - threshold 값을 정해놓고 그 값 아래에 있는 특성들을 제거한다.
        - scikit-learn에서는 VarianceThreshold()를 호출한다.
        1. Prefit with no threshold
        2. Analyze the variance
        3. Choose the threshold
        4. Refit with the chosen threshold
        
        ![image](https://user-images.githubusercontent.com/89712324/216986550-608a71c2-ef53-4941-ac84-556180bc8d68.png)
        
        - threshold=0.6으로 정하면 첫번째 열과 세번째 열이 선택된다.
            
            ![image](https://user-images.githubusercontent.com/89712324/216986628-c06917a4-cc15-4c56-b15c-b47bd29d41a5.png)
            
    - sorting by a correlation coefficient
        - 가장 흔하게 사용되는 -1과 1사이의 수로 이루어진 행렬이다.
        
        ![image](https://user-images.githubusercontent.com/89712324/216986664-fcb903e0-4bc8-452e-a8e9-c0c4b570aa4f.png)
        
        - 특정 변수 값에 해당하는 상관계수를 계산할 수 있다.
            
            ![image](https://user-images.githubusercontent.com/89712324/216986722-67f11ad2-5444-4e53-933f-879cf55dccf3.png)
            

### Wrapper methods

다양한 특성 조합들을 샘플링한 후 최적의 결과를 내는 조합을 선택한다.

- 특성의 개수가 0개에서부터 특성을 하나씩 추가하여 성능 결과를 확인하고 이를 반복한다.
- 또는 가능한 모든 특성의 개수에서 특성을 하나씩 제외하여 성능 결과를 확인하고 이를 반복한다.
    
    ![image](https://user-images.githubusercontent.com/89712324/216986768-de45aa03-6a8b-49ba-8afc-c5a495b99c1e.png)
    

### Transformation

가장 흔하게 사용하는 차원 축소 방법이다.

1. Principal Component Analysis (PCA)
    - 라벨 없이 데이터가 가장 넓게 퍼질 수 있도록 하는 특성을 생성한다.
    - 분산을 가장 크게하는 벡터들을 eigenvectors 라고 부르며, 데이터의 주성분이다.
    
    ![image](https://user-images.githubusercontent.com/89712324/216986838-7981e90c-31da-4e7d-9cf5-f88a6a14f6d6.png)
    
    - PCA를 적용하기 전과 후를 비교해보자
        
        ![image](https://user-images.githubusercontent.com/89712324/216987030-c17ed230-06ea-42b3-84fb-3bd3fada7607.png)
        
2. Linear Discriminant Analysis (LDA)
    - 라벨을 활용하여 라벨을 가장 잘 분류할 수 있는 방향으로 차원을 축소하는 방법이다.
    
    ![image](https://user-images.githubusercontent.com/89712324/216988513-592576b0-264e-4178-9ad7-508b6c0a870d.png)
    
    - LDA 적용 전과 후를 비교해보자
        
        ![image](https://user-images.githubusercontent.com/89712324/216988564-2bbe3f76-b4e0-4448-9125-434030f5095a.png)